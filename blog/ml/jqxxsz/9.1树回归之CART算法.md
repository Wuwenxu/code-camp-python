# 《机器学习实战》9.1树回归之CART算法

> 搜索微信公众号:'AI-ming3526'或者'计算机视觉这件小事' 获取更多人工智能、机器学习干货  
> csdn：https://blog.csdn.net/baidu_31657889/  
> github：https://github.com/aimi-cn/AILearners

本文出现的所有代码，均可在github上下载，不妨来个Star把谢谢~：[Github代码地址](https://github.com/aimi-cn/AILearners/tree/master/src/py2.x/ml/jqxxsz/9.RegTrees)

# 一、引言

本篇文章将会讲解CART算法的实现，通过测试不同的数据集，学习CART算法。

# 二、将CART（Classification And Regression Trees 分类回归树）算法用于回归

在之前的文章，我们学习了决策树的原理和代码实现，使用使用决策树进行分类。决策树不断将数据切分成小数据集，直到所有目标标量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，它要在给定时间内做出最佳选择，但不关心能否达到全局最优。

### 1、ID3算法的弊端

回忆一下，决策树的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。

除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征离散化，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在特性。

### 2、CART算法

与ID3算法相反，CART算法正好适用于连续型特征。CART算法使用二元切分法来处理连续型变量。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。

**CART算法有两步：**

- 决策树生成：递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义"最好"。
- 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。

决策树剪枝我们先不管，我们看下决策树生成。

在决策树的文章中，我们先根据信息熵的计算找到最佳特征切分数据集构建决策树。CART算法的决策树生成也是如此，实现过程如下：

- 使用CART算法选择特征
- 根据特征切分数据集合
- 构建树

### 3、根据特征切分数据集合

我们先找软柿子捏，CART算法这里涉及到算法，实现起来复杂些，我们先挑个简单的，即根据特征切分数据集合。创建regTrees.py文件，编写代码如下：

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
import numpy as np

'''
@description: 根据特征切分数据集合
@param: dataSet - 数据集合
        feature - 带切分的特征
        value - 该特征的值
@return: mat0 - 切分的数据集合0
        mat1 - 切分的数据集合1
'''
def binSplitDataSet(dataSet, feature, value):
    mat0 = dataSet[np.nonzero(dataSet[:,feature] > value)[0],:]
    mat1 = dataSet[np.nonzero(dataSet[:,feature] <= value)[0],:]
    return mat0, mat1

if __name__ == "__main__":
    testMat = np.mat(np.eye(4))
    mat0, mat1 = binSplitDataSet(testMat, 1, 0.5)
    print('原始集合:\n', testMat)
    print('mat0:\n', mat0)
    print('mat1:\n', mat1)
```

运行结果如下图所示：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_01.png)

我们先创建一个单位矩阵，然后根据切分规则，对数据矩阵进行切分。可以看到binSplitDataSet函数根据特定规则，对数据矩阵进行切分。

现在OK了，我们已经可以根据特征和特征值对数据进行切分了，mat0存放的是大于指定特征值的矩阵，mat1存放的是小于指定特征值的矩阵。接下来，我们就看看如何使用CART算法选择最佳分类特征。

### 4、CART算法

开发流程

```
收集数据：采用任意方法收集数据
准备数据：需要数值型数据，标称型数据应该映射成二值型数据
分析数据：绘出数据的二维可视化显示结果，以字典方式生成树
训练算法：大部分时间都花费在叶节点树模型的构建上
测试算法：使用测试数据上的R^2值来分析模型的效果
使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情
```

工作原理

1、找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下:

```
对每个特征:
    对每个特征值: 
        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）
        计算切分的误差
        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差
返回最佳切分的特征和阈值
```

2、树构建算法，函数 createTree() 伪代码大致如下:

```
找到最佳的待切分特征:
    如果该节点不能再分，将该节点存为叶节点
    执行二元切分
    在右子树调用 createTree() 方法
    在左子树调用 createTree() 方法
```

我们可以在简单数据集上先看下CART算法的实现

ex00.txt文件中存储的数据格式如下[下载地址](https://github.com/aimi-cn/AILearners/tree/master/data/ml/jqxxsz/9.RegTrees/ex00.txt):

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_02.png)

如上图所示，数据是2维的。先看下数据的分布情况，在regTrees.py文件下编写代码如下：

```python
'''
@description: 加载数据
@param: fileName - 文件名 
@return: dataMat - 数据矩阵
'''
def loadDataSet(fileName):
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = list(map(float, curLine))                    #转化为float类型
        dataMat.append(fltLine)
    return dataMat

'''
@description: 绘制数据集
@paramL filename - 文件名
@return: None
'''
def plotDataSet(filename):
    dataMat = loadDataSet(filename)                                        #加载数据集
    n = len(dataMat)                                                    #数据个数
    xcord = []; ycord = []                                                #样本点
    for i in range(n):                                                    
        xcord.append(dataMat[i][0]); ycord.append(dataMat[i][1])        #样本点
    fig = plt.figure()
    ax = fig.add_subplot(111)                                            #添加subplot
    ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                #绘制样本点
    plt.title('DataSet')                                                #绘制title
    plt.xlabel('X')
    plt.show()

if __name__ == "__main__":
    filename = 'C:\\Users\\Administrator\\Desktop\\blog\\github\\AILearners\\data\\ml\\jqxxsz\\9.RegTrees\\ex00.txt'
    plotDataSet(filename)
```

运行代码如下：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_03.png)

可以看到，这是一个很简单的数据集，我们先利用这个数据集测试我们的CART算法。

现在在regTrees.py文件下编写代码如下：

```python
'''
@description: 生成叶结点
@param: dataSet - 数据集合
@return: 目标变量的均值
'''
def regLeaf(dataSet):
    return np.mean(dataSet[:,-1])

'''
@description: 误差估计函数
@param: dataSet - 数据集合
@return: 目标变量的总方差
'''
def regErr(dataSet):
    return np.var(dataSet[:,-1]) * np.shape(dataSet)[0]

'''
@description: 找到数据的最佳二元切分方式函数
@param:  dataSet - 数据集合
        leafType - 生成叶结点
        regErr - 误差估计函数
        ops - 用户定义的参数构成的元组
@return:  bestIndex - 最佳切分特征
        bestValue - 最佳特征值
'''
def chooseBestSplit(dataSet, leafType = regLeaf, errType = regErr, ops = (1,4)):
    import types
    #tolS允许的误差下降值,tolN切分的最少样本数
    tolS = ops[0]; tolN = ops[1]
    #如果当前所有值相等,则退出。(根据set的特性)
    if len(set(dataSet[:,-1].T.tolist()[0])) == 1:
        return None, leafType(dataSet)
    #统计数据集合的行m和列n
    m, n = np.shape(dataSet)
    #默认最后一个特征为最佳切分特征,计算其误差估计
    S = errType(dataSet)
    #分别为最佳误差,最佳特征切分的索引值,最佳特征值
    bestS = float('inf'); bestIndex = 0; bestValue = 0
    #遍历所有特征列
    for featIndex in range(n - 1):
        #遍历所有特征值
        for splitVal in set(dataSet[:,featIndex].T.A.tolist()[0]):
            #根据特征和特征值切分数据集
            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)
            #如果数据少于tolN,则退出
            if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN): continue
            #计算误差估计
            newS = errType(mat0) + errType(mat1)
            #如果误差估计更小,则更新特征索引值和特征值
            if newS < bestS:
                bestIndex = featIndex
                bestValue = splitVal
                bestS = newS
    #如果误差减少不大则退出
    if (S - bestS) < tolS:
        return None, leafType(dataSet)
    #根据最佳的切分特征和特征值切分数据集合
    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)
    #如果切分出的数据集很小则退出
    if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN):
        return None, leafType(dataSet)
    #返回最佳切分特征和特征值
    return bestIndex, bestValue

if __name__ == "__main__":
    myDat = loadDataSet('C:\\Users\\Administrator\\Desktop\\blog\\github\\AILearners\\data\\ml\\jqxxsz\\9.RegTrees\\ex00.txt')
    myMat = np.mat(myDat)
    feat, val = chooseBestSplit(myMat, regLeaf, regErr, (1, 4))
    print(feat)
    print(val)
```

运行结果如下图所示：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_04.png)

可以看到，切分的最佳特征为第1列特征，最佳切分特征值为0.48813，这个特征值怎么选出来的？就是根据误差估计的大小，我们选择的这个特征值可以使误差最小化。

切分的特征和特征值我们已经选择好了，接下来就是利用选出的这两个变量创建回归树了。

创建方法很简单，我们根据切分的特征和特征值切分出两个数据集，然后将两个数据集分别用于左子树的构建和右子树的构建，直到无法找到切分的特征为止。因此，我们可以使用递归实现这个过程，在regTrees.py文件下编写代码如下：

```python
'''
@description: 树构建函数
@param: dataSet - 数据集合
        leafType - 建立叶结点的函数
        errType - 误差计算函数
        ops - 包含树构建所有其他参数的元组
@return: retTree - 构建的回归树
'''
def createTree(dataSet, leafType = regLeaf, errType = regErr, ops = (1, 4)):
    #选择最佳切分特征和特征值
    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)
    #r如果没有特征,则返回特征值
    if feat == None: return val
    #回归树
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    #分成左数据集和右数据集
    lSet, rSet = binSplitDataSet(dataSet, feat, val)
    #创建左子树和右子树
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree  

if __name__ == "__main__":
    myDat = loadDataSet('C:\\Users\\Administrator\\Desktop\\blog\\github\\AILearners\\data\\ml\\jqxxsz\\9.RegTrees\\ex00.txt')
    myMat = np.mat(myDat)
    print(createTree(myMat))
```

运行结果如下图所示：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_05.png)

从上图可知，这棵树只有两个叶结点。叶结点也就是没有子节点的节点，子节点是相对于父节点来说的，它是父节点的下一层节点。这里的两个叶节点也就是left和right了。

我们换一个复杂一点的数据集，分段常数数据集。

ex0.txt文件中存储的数据格式如下[下载地址](https://github.com/aimi-cn/AILearners/tree/master/data/ml/jqxxsz/9.RegTrees/ex0.txt):

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_06.png)

第一列的数据都是1.0，为了可视化方便，我们将第2列作为x轴数据，第3列作为y轴数据。对数据进行可视化，编写代码如下：

```python
'''
@description: 绘制ex0.txt数据集
@paramL filename - 文件名
@return: None
'''
def plotDataSet1(filename):
    dataMat = loadDataSet(filename)                                        #加载数据集
    n = len(dataMat)                                                    #数据个数
    xcord = []; ycord = []                                                #样本点
    for i in range(n):                                                    
        xcord.append(dataMat[i][1]); ycord.append(dataMat[i][2])        #样本点
    fig = plt.figure()
    ax = fig.add_subplot(111)                                            #添加subplot
    ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                #绘制样本点
    plt.title('DataSet')                                                #绘制title
    plt.xlabel('X')
    plt.show()

if __name__ == "__main__":
    filename = 'C:\\Users\\Administrator\\Desktop\\blog\\github\\AILearners\\data\\ml\\jqxxsz\\9.RegTrees\\ex0.txt'
    plotDataSet1(filename)
```

运行结果如图下所示：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_07.png)

可以看到，这个数据集是分段的。我们针对此数据集创建回归树。代码同上，运行结果如下图所示：

![](../../../img/ml/jqxxsz/9.RegTrees/ml_9_1_08.png)

可以看到，该数的结构中包含5个叶结点。

现在为止，已经完成回归树的构建，但是需要某种措施来检查构建过程是否得当。这个技术就是剪枝（tree pruning）技术,下节我们就来揭开他的面纱。